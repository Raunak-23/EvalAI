{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/Raunak-23/EvalAI/blob/dev/ocr_pytorch.ipynb",
      "authorship_tag": "ABX9TyO1ixfRvKZYsvk52M44+ITE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raunak-23/EvalAI/blob/main/ml/ocr/ocr_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xK4mF975r2H",
        "outputId": "f9256dd5-3a79-4d7a-c0c0-197482c5a258"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a93428b4",
        "outputId": "2dd0ce81-824c-4668-b07d-35da45e37e2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rn8BlkEcudBk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from os import walk\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage.measure import find_contours\n",
        "from skimage.morphology import binary_dilation\n",
        "from sklearn.svm import SVC\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters and constants\n",
        "AVAILABLE_WRITERS = 672\n",
        "RESULTS_FILE = 'results.txt'\n",
        "TIME_FILE = 'time.txt'\n",
        "OVERLAPPING_METHOD = 0\n",
        "LINES_METHOD = 1\n",
        "SUPPORT_VECTOR_CLASSIFIER = 0\n",
        "NEURAL_NETWORK_CLASSIFIER = 1\n",
        "HISTOGRAM_BINS = 256\n",
        "NN_LEARNING_RATE = 0.003\n",
        "NN_WEIGHT_DECAY = 0.01\n",
        "NN_DROPOUT = 0.25\n",
        "NN_EPOCHS = 200\n",
        "NN_BATCH_SIZE = 16\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "KPmn5wsa55NA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(images, titles=None):\n",
        "    n_ims = len(images)\n",
        "    if titles is None:\n",
        "        titles = ['(%d)' % i for i in range(1, n_ims + 1)]\n",
        "    fig = plt.figure()\n",
        "    n = 1\n",
        "    for image, title in zip(images, titles):\n",
        "        a = fig.add_subplot(1, n_ims, n)\n",
        "        if image.ndim == 2:\n",
        "            plt.gray()\n",
        "        plt.imshow(image)\n",
        "        a.set_title(title)\n",
        "        n += 1\n",
        "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZjAZejbo59kE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(img, feature_extraction_method=OVERLAPPING_METHOD):\n",
        "    if feature_extraction_method == OVERLAPPING_METHOD:\n",
        "        img_copy = img.copy()\n",
        "        if len(img.shape) > 2:\n",
        "            img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n",
        "        img_copy = cv2.medianBlur(img_copy, 5)\n",
        "        img_copy = cv2.threshold(img_copy, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "        min_vertical, max_vertical = get_corpus_boundaries(img_copy)\n",
        "        img_copy = img_copy[min_vertical:max_vertical]\n",
        "        return img_copy\n",
        "\n",
        "    if feature_extraction_method == LINES_METHOD:\n",
        "        img_copy = img.copy()\n",
        "        if len(img.shape) > 2:\n",
        "            grayscale_img = cv2.cvtColor(img_copy, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            grayscale_img = img.copy()\n",
        "        img_copy = cv2.threshold(grayscale_img, 127, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "        min_vertical, max_vertical = get_corpus_boundaries(img_copy)\n",
        "        img_copy = img_copy[min_vertical:max_vertical]\n",
        "        grayscale_img = grayscale_img[min_vertical:max_vertical]\n",
        "        filter_kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
        "        img_copy_sharpened = cv2.filter2D(img_copy, -1, filter_kernel)\n",
        "        return img_copy_sharpened, grayscale_img"
      ],
      "metadata": {
        "id": "CB0o2S946DZl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_corpus_boundaries(img):\n",
        "    crop = []\n",
        "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (100, 1))\n",
        "    detect_horizontal = cv2.morphologyEx(img, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\n",
        "    contours = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours = contours[0] if len(contours) == 2 else contours[1]\n",
        "    prev = -1\n",
        "    for i, c in enumerate(contours):\n",
        "        if np.abs(prev - int(c[0][0][1])) > 800 or prev == -1:\n",
        "            crop.append(int(c[0][0][1]))\n",
        "            prev = int(c[0][0][1])\n",
        "    crop.sort()\n",
        "    max_vertical = crop[1] - 20\n",
        "    min_vertical = crop[0] + 20\n",
        "    return min_vertical, max_vertical"
      ],
      "metadata": {
        "id": "IZNT2Ygf6HLh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_image(img, num, grayscale_img=None):\n",
        "    if grayscale_img is not None:\n",
        "        grayscale_images = []\n",
        "        img_copy = np.copy(img)\n",
        "        kernel = np.ones((1, num))\n",
        "        img_copy = binary_dilation(img_copy, kernel)\n",
        "        bounding_boxes = find_contours(img_copy, 0.8)\n",
        "        for box in bounding_boxes:\n",
        "            x_min = int(np.min(box[:, 1]))\n",
        "            x_max = int(np.max(box[:, 1]))\n",
        "            y_min = int(np.min(box[:, 0]))\n",
        "            y_max = int(np.max(box[:, 0]))\n",
        "            if (y_max - y_min) > 50 and (x_max - x_min) > 50:\n",
        "                grayscale_images.append(grayscale_img[y_min:y_max, x_min:x_max])\n",
        "        return grayscale_images\n",
        "    images = []\n",
        "    img_copy = np.copy(img)\n",
        "    kernel = np.ones((1, num))\n",
        "    img_copy = binary_dilation(img_copy, kernel)\n",
        "    bounding_boxes = find_contours(img_copy, 0.8)\n",
        "    for box in bounding_boxes:\n",
        "        x_min = int(np.min(box[:, 1]))\n",
        "        x_max = int(np.max(box[:, 1]))\n",
        "        y_min = int(np.min(box[:, 0]))\n",
        "        y_max = int(np.max(box[:, 0]))\n",
        "        if (y_max - y_min) > 10 and (x_max - x_min) > 10:\n",
        "            images.append(img[y_min:y_max, x_min:x_max])\n",
        "    return images"
      ],
      "metadata": {
        "id": "kjjkaUsA6JxF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overlap_words(words, avg_height):\n",
        "    overlapped_img = np.zeros((3600, 320))\n",
        "    index_i = 0\n",
        "    index_j = 0\n",
        "    max_height = 0\n",
        "    for word in words:\n",
        "        if word.shape[1] + index_j > overlapped_img.shape[1]:\n",
        "            max_height = 0\n",
        "            index_j = 0\n",
        "            index_i += int(avg_height // 2)\n",
        "        if word.shape[1] < overlapped_img.shape[1] and word.shape[0] < overlapped_img.shape[0]:\n",
        "            indices = np.copy(overlapped_img[index_i:index_i + word.shape[0], index_j:index_j + word.shape[1]])\n",
        "            indices = np.maximum(indices, word)\n",
        "            overlapped_img[index_i:index_i + word.shape[0], index_j:index_j + word.shape[1]] = indices\n",
        "            index_j += word.shape[1]\n",
        "            if max_height < word.shape[0]:\n",
        "                max_height = word.shape[0]\n",
        "    overlapped_img = overlapped_img[:index_i + int(avg_height // 2), :]\n",
        "    return overlapped_img"
      ],
      "metadata": {
        "id": "VpLMzhNm6OK-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_textures(image):\n",
        "    index_i = 0\n",
        "    index_j = 0\n",
        "    texture_size = 100\n",
        "    textures = []\n",
        "    while index_i + texture_size < image.shape[0]:\n",
        "        if index_j + texture_size > image.shape[1]:\n",
        "            index_j = 0\n",
        "            index_i += texture_size\n",
        "        textures.append(np.copy(image[index_i: index_i + texture_size, index_j: index_j + texture_size]))\n",
        "        index_j += texture_size\n",
        "    return textures"
      ],
      "metadata": {
        "id": "8TvnTEKb6Q7t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_generator(features, labels, feature_extraction_method=OVERLAPPING_METHOD,\n",
        "                    classifier_type=SUPPORT_VECTOR_CLASSIFIER):\n",
        "    histograms = []\n",
        "\n",
        "    if feature_extraction_method == OVERLAPPING_METHOD:\n",
        "        for texture_array in features:\n",
        "            for texture in texture_array:\n",
        "                lbp = local_binary_pattern(texture, 8, 3, 'default')\n",
        "                histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n",
        "                histograms.append(histogram)\n",
        "\n",
        "    elif feature_extraction_method == LINES_METHOD:\n",
        "        for line in features:\n",
        "            lbp = local_binary_pattern(line, 8, 3, 'default')\n",
        "            histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n",
        "            histograms.append(histogram)\n",
        "\n",
        "    if classifier_type == SUPPORT_VECTOR_CLASSIFIER:\n",
        "        model = SVC(kernel='linear')\n",
        "        model.fit(histograms, labels)\n",
        "        return model\n",
        "\n",
        "    if classifier_type == NEURAL_NETWORK_CLASSIFIER:\n",
        "        model = nn.Sequential(nn.Linear(HISTOGRAM_BINS, 128),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Dropout(p=NN_DROPOUT),\n",
        "                              nn.Linear(128, 64),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Dropout(p=NN_DROPOUT),\n",
        "                              nn.Linear(64, 3))\n",
        "        model.to(DEVICE)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adamax(model.parameters(), lr=NN_LEARNING_RATE, weight_decay=NN_WEIGHT_DECAY)\n",
        "        inputs = torch.Tensor(histograms)\n",
        "        labels = torch.tensor(labels, dtype=torch.long) - 1\n",
        "        dataset = TensorDataset(inputs, labels)\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=NN_BATCH_SIZE, shuffle=True)\n",
        "        for epoch in range(NN_EPOCHS):\n",
        "            for inputs, labels in train_loader:\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                output = model(inputs)\n",
        "                loss = criterion(output, labels)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return model"
      ],
      "metadata": {
        "id": "-xDpeLw06USy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, test_image, feature_extraction_method=OVERLAPPING_METHOD, classifier_type=SUPPORT_VECTOR_CLASSIFIER):\n",
        "    if feature_extraction_method == OVERLAPPING_METHOD:\n",
        "        img = preprocess_image(test_image)\n",
        "        words = segment_image(img, 3)\n",
        "        avg_height = 0\n",
        "        for word in words:\n",
        "            avg_height += word.shape[0] / len(words)\n",
        "        overlapped_img = overlap_words(words, avg_height)\n",
        "        textures = get_textures(overlapped_img)\n",
        "        prediction = np.zeros(4)\n",
        "        for texture in textures:\n",
        "            lbp = local_binary_pattern(texture, 8, 3, 'default')\n",
        "            histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n",
        "            if classifier_type == SUPPORT_VECTOR_CLASSIFIER:\n",
        "                prediction[model.predict([histogram])] += 1\n",
        "            if classifier_type == NEURAL_NETWORK_CLASSIFIER:\n",
        "                with torch.no_grad():\n",
        "                    model.eval()\n",
        "                    histogram = torch.Tensor(histogram)\n",
        "                    probabilities = F.softmax(model.forward(histogram), dim=0)\n",
        "                    _, top_class = probabilities.topk(1)\n",
        "                    prediction[top_class + 1] += 1\n",
        "        return np.argmax(prediction)\n",
        "\n",
        "    if feature_extraction_method == LINES_METHOD:\n",
        "        img, grayscale_img = preprocess_image(test_image, feature_extraction_method)\n",
        "        grayscale_lines = segment_image(img, 100, grayscale_img)\n",
        "        prediction = np.zeros(4)\n",
        "        for line in grayscale_lines:\n",
        "            lbp = local_binary_pattern(line, 8, 3, 'default')\n",
        "            histogram, _ = np.histogram(lbp, density=False, bins=HISTOGRAM_BINS, range=(0, HISTOGRAM_BINS))\n",
        "            if classifier_type == SUPPORT_VECTOR_CLASSIFIER:\n",
        "                prediction[model.predict([histogram])] += 1\n",
        "            if classifier_type == NEURAL_NETWORK_CLASSIFIER:\n",
        "                with torch.no_grad():\n",
        "                    model.eval()\n",
        "                    histogram = torch.Tensor(histogram)\n",
        "                    probabilities = F.softmax(model.forward(histogram), dim=0)\n",
        "                    _, top_class = probabilities.topk(1)\n",
        "                    prediction[top_class + 1] += 1\n",
        "        return np.argmax(prediction)"
      ],
      "metadata": {
        "id": "sjF4c-jx6Yj8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_random_images(root):\n",
        "    images = []\n",
        "    labels = []\n",
        "    test_images = []\n",
        "    test_labels = []\n",
        "    for i in range(3):\n",
        "        found_images = False\n",
        "        while not found_images:\n",
        "            images_path = root\n",
        "            random_writer = random.randrange(AVAILABLE_WRITERS)\n",
        "            if random_writer < 10:\n",
        "                random_writer = \"00\" + str(random_writer)\n",
        "            elif random_writer < 100:\n",
        "                random_writer = \"0\" + str(random_writer)\n",
        "            images_path = os.path.join(images_path, str(random_writer))\n",
        "            if not os.path.isdir(images_path):\n",
        "                continue\n",
        "            _, _, filenames = next(walk(images_path))\n",
        "            if len(filenames) <= 2 and i == 2 and len(test_images) == 0:\n",
        "                continue\n",
        "            if len(filenames) >= 2:\n",
        "                found_images = True\n",
        "                chosen_filenames = []\n",
        "                for j in range(2):\n",
        "                    random_filename = random.choice(filenames)\n",
        "                    while random_filename in chosen_filenames:\n",
        "                        random_filename = random.choice(filenames)\n",
        "                    chosen_filenames.append(random_filename)\n",
        "                    images.append(cv2.imread(os.path.join(images_path, random_filename)))\n",
        "                    labels.append(i + 1)\n",
        "                if len(filenames) >= 3:\n",
        "                    random_filename = random.choice(filenames)\n",
        "                    while random_filename in chosen_filenames:\n",
        "                        random_filename = random.choice(filenames)\n",
        "                    chosen_filenames.append(random_filename)\n",
        "                    test_images.append(cv2.imread(os.path.join(images_path, random_filename)))\n",
        "                    test_labels.append(i + 1)\n",
        "    test_choice = random.randint(0, len(test_images) - 1)\n",
        "    test_image = test_images[test_choice]\n",
        "    test_label = test_labels[test_choice]\n",
        "    return images, labels, test_image, test_label"
      ],
      "metadata": {
        "id": "7eun5u0m6e5J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(images, labels, feature_extraction_method=OVERLAPPING_METHOD):\n",
        "    if feature_extraction_method == LINES_METHOD:\n",
        "        lines_labels = []\n",
        "        lines = []\n",
        "        for image, label in zip(images, labels):\n",
        "            image, grayscale_image = preprocess_image(image, feature_extraction_method)\n",
        "            grayscale_lines = segment_image(image, 100, grayscale_image)\n",
        "            for line in grayscale_lines:\n",
        "                lines.append(line)\n",
        "                lines_labels.append(label)\n",
        "        return lines, lines_labels\n",
        "\n",
        "    if feature_extraction_method == OVERLAPPING_METHOD:\n",
        "        textures = []\n",
        "        textures_labels = []\n",
        "        for image, label in zip(images, labels):\n",
        "            image = preprocess_image(image)\n",
        "            words = segment_image(image, 3)\n",
        "            avg_height = 0\n",
        "            for word in words:\n",
        "                avg_height += word.shape[0] / len(words)\n",
        "            overlapped_img = overlap_words(words, avg_height)\n",
        "            new_textures = get_textures(overlapped_img)\n",
        "            textures.append(new_textures)\n",
        "            for j in range(len(new_textures)):\n",
        "                textures_labels.append(label)\n",
        "        return textures, textures_labels"
      ],
      "metadata": {
        "id": "MoZMj4CT6gFd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install kagglehub"
      ],
      "metadata": {
        "id": "hAqM5GzS6k4s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccd3c81e"
      },
      "source": [
        "# import kagglehub\n",
        "\n",
        "# # Download latest version\n",
        "# path = kagglehub.dataset_download(\"naderabdalghani/iam-handwritten-forms-dataset\")\n",
        "\n",
        "# print(\"Path to dataset files:\", path)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========  PARAMETERS  ==========\n",
        "OVERLAPPING_METHOD   = 0\n",
        "SUPPORT_VECTOR_CLASSIFIER = 0\n",
        "AVAILABLE_WRITERS    = 672          # IAM has 657 writers – we take a safe upper bound\n",
        "epochs               = 10           # start small – increase later\n",
        "root                 = \"/content/drive/MyDrive/data/iam_words/words\"   # <— change if different\n",
        "# ==================================\n",
        "\n",
        "correct_predictions   = 0\n",
        "total_execution_time  = 0\n",
        "\n",
        "def collect_image_paths(root_dir, writers_needed=3, files_per_writer=2):\n",
        "    \"\"\"\n",
        "    Returns  ( [(path, label), …], test_path, test_label )\n",
        "    labels are 1,2,3  (writer index)\n",
        "    \"\"\"\n",
        "    import random, os, glob\n",
        "    writers = [d for d in os.listdir(root_dir)\n",
        "               if os.path.isdir(os.path.join(root_dir, d))]\n",
        "    random.shuffle(writers)\n",
        "    out, labels = [], []\n",
        "    for w_idx, writer in enumerate(writers[:writers_needed]):\n",
        "        writer_dir = os.path.join(root_dir, writer)\n",
        "        pages = [d for d in os.listdir(writer_dir)\n",
        "                 if os.path.isdir(os.path.join(writer_dir, d))]\n",
        "        if not pages:\n",
        "            continue\n",
        "        # pick one page for this writer\n",
        "        page = random.choice(pages)\n",
        "        page_dir = os.path.join(writer_dir, page)\n",
        "        pngs = glob.glob(os.path.join(page_dir, \"*.png\"))\n",
        "        if len(pngs) < files_per_writer + 1:        # need 2 train + 1 test\n",
        "            continue\n",
        "        sample = random.sample(pngs, files_per_writer + 1)\n",
        "        train = sample[:-1]\n",
        "        test  = sample[-1]\n",
        "        for p in train:\n",
        "            out.append(p)\n",
        "            labels.append(w_idx + 1)                 # label 1,2,3\n",
        "    if len(out) < writers_needed * files_per_writer:\n",
        "        raise RuntimeError(\"Not enough images – check folder structure\")\n",
        "    return out, labels, test, w_idx + 1\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    try:\n",
        "        train_paths, train_labels, test_path, test_label = collect_image_paths(root)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "        break\n",
        "\n",
        "    images = [cv2.imread(p) for p in train_paths]\n",
        "    start = time.time()\n",
        "\n",
        "    features, features_labels = extract_features(images, train_labels,\n",
        "                                                 feature_extraction_method=OVERLAPPING_METHOD)\n",
        "    model = model_generator(features, features_labels,\n",
        "                            feature_extraction_method=OVERLAPPING_METHOD,\n",
        "                            classifier_type=SUPPORT_VECTOR_CLASSIFIER)\n",
        "\n",
        "    test_img = cv2.imread(test_path)\n",
        "    pred = predict(model, test_img,\n",
        "                   feature_extraction_method=OVERLAPPING_METHOD,\n",
        "                   classifier_type=SUPPORT_VECTOR_CLASSIFIER)\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    total_execution_time += elapsed\n",
        "    if pred == test_label:\n",
        "        correct_predictions += 1\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d} | \"\n",
        "          f\"time {elapsed:5.2f}s | \"\n",
        "          f\"accuracy {100*correct_predictions/(epoch+1):5.2f}%\")\n",
        "\n",
        "print(\"\\nFinal accuracy : {:.2f} %  ({} epochs)\".format(\n",
        "      100 * correct_predictions / epochs, epochs))"
      ],
      "metadata": {
        "id": "SjPrHfFA6pNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2538bca3-2482-4374-8367-eefd2c059462"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not enough images – check folder structure\n",
            "\n",
            "Final accuracy : 0.00 %  (10 epochs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "flatten_iam_words.py\n",
        "Flatten the IAM-word nested folder structure.\n",
        "Before:  iam_words/words/a01/a01-000u/a01-000u-00-00.png\n",
        "After:   iam_words/words_flat/a01-000u-00-00.png\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm   # pip install tqdm  (optional, nicer progress bar)\n",
        "\n",
        "# ------------- CONFIG --------------\n",
        "SRC = Path(\"/content/drive/MyDrive/data/iam_words/words\")   # change if necessary\n",
        "DST = Path(\"/content/drive/MyDrive/data/iam_words/words_flat\")                        # same parent dir\n",
        "# -----------------------------------\n",
        "\n",
        "DST.mkdir(exist_ok=True)\n",
        "\n",
        "pngs = list(SRC.rglob(\"*.png\"))\n",
        "print(f\"Found {len(pngs)} PNG files.\")\n",
        "\n",
        "for file in tqdm(pngs, desc=\"Copying\"):\n",
        "    target = DST / file.name\n",
        "    # handle extremely unlikely name collision\n",
        "    counter = 1\n",
        "    stem = target.stem\n",
        "    suffix = target.suffix\n",
        "    while target.exists():\n",
        "        target = DST / f\"{stem}_{counter}{suffix}\"\n",
        "        counter += 1\n",
        "    shutil.copy2(file, target)\n",
        "\n",
        "print(\"Done – flat folder:\", DST.resolve())"
      ],
      "metadata": {
        "id": "hSXzZkn36mLb",
        "outputId": "46428d0c-17ec-4895-ac80-43d9fbce2c9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 63766 PNG files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying:  15%|█▌        | 9632/63766 [46:26<4:21:02,  3.46it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2964816213.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDST\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{stem}_{counter}{suffix}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done – flat folder:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}